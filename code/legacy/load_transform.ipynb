{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "355c209d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"e93105b4-214b-4b5f-8efc-d9a01e2192e1\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"e93105b4-214b-4b5f-8efc-d9a01e2192e1\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.4.3.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"e93105b4-214b-4b5f-8efc-d9a01e2192e1\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import hail as hl\n",
    "import pandas as pd\n",
    "# Parameters\n",
    "log_path = '../logs/hail'\n",
    "file_in = '../data/multisample_espanha.vcf.gz'\n",
    "file_out = '../data/multisample_espanha.mt'\n",
    "reference_genome = 'GRCh37' #'GRCh38'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9a63a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/04 08:29:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Running on Apache Spark version 3.5.7\n",
      "SparkUI available at http://pimenta:4040\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.136-c32f88309ab0\n",
      "LOGGING: writing to ../logs/hail\n"
     ]
    }
   ],
   "source": [
    "hl.init(app_name = 'gnomAD', log = log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48cbf36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 08:30:12.094 Hail: INFO: scanning VCF for sortedness...\n",
      "2025-11-04 08:30:16.663 Hail: INFO: Coerced sorted VCF - no additional import work to do\n",
      "2025-11-04 08:30:36.566 Hail: INFO: wrote matrix table with 708424 rows and 267 columns in 2 partitions to ../data/multisample_espanha.mt\n"
     ]
    }
   ],
   "source": [
    "# Import and write matrix table\n",
    "mt = hl.import_vcf(file_in, force_bgz=True, reference_genome=reference_genome)\n",
    "mt.write(file_out, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36f5470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = hl.read_matrix_table(file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91c9d427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix table schema:\n",
      "----------------------------------------\n",
      "Global fields:\n",
      "    None\n",
      "----------------------------------------\n",
      "Column fields:\n",
      "    's': str\n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'locus': locus<GRCh37>\n",
      "    'alleles': array<str>\n",
      "    'rsid': str\n",
      "    'qual': float64\n",
      "    'filters': set<str>\n",
      "    'info': struct {\n",
      "        AC: array<int32>, \n",
      "        AF: array<float64>, \n",
      "        AN: int32, \n",
      "        BaseQRankSum: float64, \n",
      "        DP: int32, \n",
      "        DS: bool, \n",
      "        Dels: float64, \n",
      "        FS: float64, \n",
      "        HaplotypeScore: float64, \n",
      "        InbreedingCoeff: float64, \n",
      "        MLEAC: array<int32>, \n",
      "        MLEAF: array<float64>, \n",
      "        MQ: float64, \n",
      "        MQ0: int32, \n",
      "        MQRankSum: float64, \n",
      "        QD: float64, \n",
      "        RPA: array<int32>, \n",
      "        RU: str, \n",
      "        ReadPosRankSum: float64, \n",
      "        STR: bool\n",
      "    }\n",
      "----------------------------------------\n",
      "Entry fields:\n",
      "    'AD': array<int32>\n",
      "    'DP': int32\n",
      "    'GQ': int32\n",
      "    'GT': call\n",
      "    'PL': array<int32>\n",
      "----------------------------------------\n",
      "Column key: ['s']\n",
      "Row key: ['locus', 'alleles']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Here need to be changed in the real dataset. The current dataset is just a mockup.\n",
    "# For example, adding annotations from gnomAD, filtering samples, etc.\n",
    "# # Calculate basic allele statistics (similar to gnomAD)\n",
    "# mt = hl.variant_qc(mt)\n",
    "print(\"Matrix table schema:\")\n",
    "mt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "137c0847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Available INFO fields ===\n",
      "['AC', 'AF', 'AN', 'BaseQRankSum', 'DP', 'DS', 'Dels', 'FS', 'HaplotypeScore', 'InbreedingCoeff', 'MLEAC', 'MLEAF', 'MQ', 'MQ0', 'MQRankSum', 'QD', 'RPA', 'RU', 'ReadPosRankSum', 'STR']\n"
     ]
    }
   ],
   "source": [
    "# Get available info fields\n",
    "info_fields = list(mt.info.dtype.fields)\n",
    "print(f\"\\n=== Available INFO fields ===\")\n",
    "print(info_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79e0cfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract gnomAD statistics from info field\n",
    "variants = mt.rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "272e6947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_field(field_name, default_value, is_array=False, index=0):\n",
    "    \"\"\"\n",
    "    Safely extract info field, handling missing fields and missing arrays\n",
    "    by coalescing to a default value.\n",
    "    \"\"\"\n",
    "    if field_name not in info_fields:\n",
    "        return hl.literal(default_value)  # Field doesn't exist in header\n",
    "    \n",
    "    field = variants.info[field_name]\n",
    "    \n",
    "    if is_array:\n",
    "        # 1. Check if field is defined.\n",
    "        # 2. If yes, check if length is > index.\n",
    "        # 3. If yes, get element.\n",
    "        # 4. If no to 1 or 2, return default.\n",
    "        return hl.if_else(\n",
    "            hl.is_defined(field) & (hl.len(field) > index),\n",
    "            field[index],\n",
    "            hl.literal(default_value)\n",
    "        )\n",
    "    else:\n",
    "        # Coalesce returns the field if it's not missing,\n",
    "        # otherwise it returns the default value.\n",
    "        return hl.coalesce(field, hl.literal(default_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "406edb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build selection dict dynamically\n",
    "selection = {\n",
    "    # Locus information (always present)\n",
    "    'chrom': variants.locus.contig,\n",
    "    'pos': variants.locus.position,\n",
    "    'ref': variants.alleles[0],\n",
    "    'alt': hl.delimit(variants.alleles[1:], ','),\n",
    "    'rsid': hl.coalesce(variants.rsid, hl.missing(hl.tstr)), # rsid can also be missing\n",
    "    'qual': variants.qual,\n",
    "    'filters': hl.delimit(variants.filters, ','),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e1c56b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnomAD-specific fields (may not be present)\n",
    "gnomad_fields = {\n",
    "    'grpmax': ('grpmax', '', True),\n",
    "    'faf95_max': ('fafmax_faf95_max', 0.0, True),\n",
    "    'faf95_max_gen_anc': ('fafmax_faf95_max_gen_anc', '', True),\n",
    "    'nhomalt': ('nhomalt', 0, True),\n",
    "    \n",
    "    # Sex-specific\n",
    "    'AC_XX': ('AC_XX', 0, True),\n",
    "    'AN_XX': ('AN_XX', 0, False),\n",
    "    'AF_XX': ('AF_XX', 0.0, True),\n",
    "    'nhomalt_XX': ('nhomalt_XX', 0, True),\n",
    "    'AC_XY': ('AC_XY', 0, True),\n",
    "    'AN_XY': ('AN_XY', 0, False),\n",
    "    'AF_XY': ('AF_XY', 0.0, True),\n",
    "    'nhomalt_XY': ('nhomalt_XY', 0, True),\n",
    "}\n",
    "for new_name, (field, default, is_array) in gnomad_fields.items():\n",
    "    selection[new_name] = get_info_field(field, default, is_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98d9ea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population-specific fields\n",
    "populations = ['afr', 'amr', 'asj', 'eas', 'fin', 'nfe', 'sas', 'mid']\n",
    "for pop in populations:\n",
    "    gnomad_fields[f'AC_{pop}'] = (f'AC_{pop}', 0, True)\n",
    "    gnomad_fields[f'AF_{pop}'] = (f'AF_{pop}', 0.0, True)\n",
    "    gnomad_fields[f'AN_{pop}'] = (f'AN_{pop}', 0, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "caaee82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction scores\n",
    "prediction_fields = {\n",
    "    'cadd_phred': ('cadd_phred', hl.float64(0.0), False),\n",
    "    'cadd_raw_score': ('cadd_raw_score', hl.float64(0.0), False),\n",
    "    'revel_max': ('revel_max', hl.float64(0.0), False),\n",
    "    'spliceai_ds_max': ('spliceai_ds_max', hl.float64(0.0), False),\n",
    "    'phylop': ('phylop', hl.float64(0.0), False),\n",
    "    'sift_max': ('sift_max', hl.float64(0.0), False),\n",
    "    'polyphen_max': ('polyphen_max', hl.float64(0.0), False),\n",
    "}\n",
    "for new_name, (field, default, is_array) in prediction_fields.items():\n",
    "    selection[new_name] = get_info_field(field, default, is_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8bcc8b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality metrics (common in VCFs)\n",
    "quality_fields = {\n",
    "    'DP': 0, 'AN': 0, # Integers\n",
    "    'FS': 0.0, 'MQ': 0.0, 'MQRankSum': 0.0, 'QD': 0.0, \n",
    "    'ReadPosRankSum': 0.0, 'SOR': 0.0, 'BaseQRankSum': 0.0, \n",
    "    'InbreedingCoeff': 0.0 # Floats\n",
    "}\n",
    "for field, default in quality_fields.items():\n",
    "    selection[field] = get_info_field(field, default, is_array=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd25e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add gnomAD fields if they exist\n",
    "# for new_name, (field_name, default, is_array) in gnomad_fields.items():\n",
    "#     if field_name in info_fields:\n",
    "#         selection[new_name] = get_info_field(variants, field_name, default, is_array)\n",
    "\n",
    "# # Add prediction fields if they exist\n",
    "# for new_name, (field_name, default, is_array) in prediction_fields.items():\n",
    "#     if field_name in info_fields:\n",
    "#         selection[new_name] = get_info_field(variants, field_name, default, is_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba4e9187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality metrics (common in VCFs)\n",
    "quality_fields = ['DP', 'FS', 'MQ', 'MQRankSum', 'QD', 'ReadPosRankSum', 'SOR', \n",
    "                  'BaseQRankSum', 'InbreedingCoeff']\n",
    "for field in quality_fields:\n",
    "    if field in info_fields:\n",
    "        selection[field] = variants.info[field]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "584c728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VEP annotation if present\n",
    "if 'vep' in info_fields:\n",
    "    selection['vep'] = variants.info.vep # VEP is complex, handle separately\n",
    "\n",
    "# Core allele statistics (now using our robust function)\n",
    "# Note: AN is already handled by quality_fields, but this is fine\n",
    "selection['AC'] = get_info_field('AC', 0, is_array=True, index=0)\n",
    "selection['AN'] = get_info_field('AN', 0, is_array=False)\n",
    "selection['AF'] = get_info_field('AF', 0.0, is_array=True, index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c9bde785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Selected fields (38) ===\n",
      "['chrom', 'pos', 'ref', 'alt', 'rsid', 'qual', 'filters', 'grpmax', 'faf95_max', 'faf95_max_gen_anc', 'nhomalt', 'AC_XX', 'AN_XX', 'AF_XX', 'nhomalt_XX', 'AC_XY', 'AN_XY', 'AF_XY', 'nhomalt_XY', 'cadd_phred', 'cadd_raw_score', 'revel_max', 'spliceai_ds_max', 'phylop', 'sift_max', 'polyphen_max', 'DP', 'AN', 'FS', 'MQ', 'MQRankSum', 'QD', 'ReadPosRankSum', 'SOR', 'BaseQRankSum', 'InbreedingCoeff', 'AC', 'AF']\n"
     ]
    }
   ],
   "source": [
    "# Apply selection\n",
    "variants = variants.select(**selection)\n",
    "\n",
    "# Get field list for display\n",
    "selected_fields = list(selection.keys())\n",
    "print(f\"\\n=== Selected fields ({len(selected_fields)}) ===\")\n",
    "print(selected_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1d094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # helper function to extract info field\n",
    "# def first_or_default(expr, default):\n",
    "#     return hl.if_else(hl.len(expr) > 0, expr[0], default)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d06a5fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select relevant fields from the info struct\n",
    "# variants = variants.select(\n",
    "#     # Locus information\n",
    "#     chrom=variants.locus.contig,\n",
    "#     pos=variants.locus.position,\n",
    "#     ref=variants.alleles[0],\n",
    "#     alt=hl.delimit(variants.alleles[1:], ','),  # Join alt alleles if multiple\n",
    "#     rsid=variants.rsid,\n",
    "\n",
    "#     # Basic allele statistics (from gnomAD info field)\n",
    "#     AC=first_or_default(variants.info.AC, 0),\n",
    "#     AN=variants.info.AN,\n",
    "#     AF=first_or_default(variants.info.AF, 0.0),\n",
    "\n",
    "#     # Genetic ancestry group with max AF\n",
    "#     grpmax=first_or_default(variants.info.grpmax, ''),\n",
    "\n",
    "#     # Filtering Allele Frequency (95% confidence)\n",
    "#     faf95_max=first_or_default(variants.info.fafmax_faf95_max, 0.0),\n",
    "#     faf95_max_gen_anc=first_or_default(variants.info.fafmax_faf95_max_gen_anc, ''),\n",
    "\n",
    "#     # Number of homozygotes\n",
    "#     nhomalt=first_or_default(variants.info.nhomalt, 0),\n",
    "\n",
    "#     # Sex-specific counts\n",
    "#     AC_XX=first_or_default(variants.info.AC_XX, 0),\n",
    "#     AN_XX=variants.info.AN_XX,\n",
    "#     AF_XX=first_or_default(variants.info.AF_XX, 0.0),\n",
    "#     nhomalt_XX=first_or_default(variants.info.nhomalt_XX, 0),\n",
    "\n",
    "#     AC_XY=first_or_default(variants.info.AC_XY, 0),\n",
    "#     AN_XY=variants.info.AN_XY,\n",
    "#     AF_XY=first_or_default(variants.info.AF_XY, 0.0),\n",
    "#     nhomalt_XY=first_or_default(variants.info.nhomalt_XY, 0),\n",
    "\n",
    "#     # Population-specific (examples)\n",
    "#     AC_afr=first_or_default(variants.info.AC_afr, 0),\n",
    "#     AF_afr=first_or_default(variants.info.AF_afr, 0.0),\n",
    "#     AN_afr=variants.info.AN_afr,\n",
    "\n",
    "#     AC_amr=first_or_default(variants.info.AC_amr, 0),\n",
    "#     AF_amr=first_or_default(variants.info.AF_amr, 0.0),\n",
    "#     AN_amr=variants.info.AN_amr,\n",
    "\n",
    "#     AC_eas=first_or_default(variants.info.AC_eas, 0),\n",
    "#     AF_eas=first_or_default(variants.info.AF_eas, 0.0),\n",
    "#     AN_eas=variants.info.AN_eas,\n",
    "\n",
    "#     AC_nfe=first_or_default(variants.info.AC_nfe, 0),\n",
    "#     AF_nfe=first_or_default(variants.info.AF_nfe, 0.0),\n",
    "#     AN_nfe=variants.info.AN_nfe,\n",
    "\n",
    "#     AC_sas=first_or_default(variants.info.AC_sas, 0),\n",
    "#     AF_sas=first_or_default(variants.info.AF_sas, 0.0),\n",
    "#     AN_sas=variants.info.AN_sas,\n",
    "\n",
    "#     # Prediction scores\n",
    "#     cadd_phred=variants.info.cadd_phred,\n",
    "#     revel_max=variants.info.revel_max,\n",
    "#     spliceai_ds_max=variants.info.spliceai_ds_max,\n",
    "\n",
    "#     # Quality metrics\n",
    "#     qual=variants.qual,\n",
    "#     filters=hl.delimit(variants.filters, ','),\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c5d81d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "ename": "FatalError",
     "evalue": "SparkException: Job aborted due to stage failure: Task 1 in stage 7.0 failed 1 times, most recent failure: Lost task 1.0 in stage 7.0 (TID 15) (pimenta executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\n\nJava stack trace:\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 7.0 failed 1 times, most recent failure: Lost task 1.0 in stage 7.0 (TID 15) (pimenta executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2898)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2834)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2833)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2833)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1253)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1253)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3102)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3036)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3025)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:995)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$parallelizeAndComputeWithIndex$4(SparkBackend.scala:344)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$parallelizeAndComputeWithIndex$4$adapted(SparkBackend.scala:343)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat is.hail.backend.spark.SparkBackend.parallelizeAndComputeWithIndex(SparkBackend.scala:343)\n\tat is.hail.backend.BackendUtils.collectDArray(BackendUtils.scala:85)\n\tat __C4691Compile.apply_region242_242(Emit.scala)\n\tat __C4691Compile.apply_region240_242(Emit.scala)\n\tat __C4691Compile.apply(Emit.scala)\n\tat is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$5(CompileAndEvaluate.scala:93)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:99)\n\tat is.hail.backend.ExecuteContext.time(ExecuteContext.scala:174)\n\tat is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$4(CompileAndEvaluate.scala:93)\n\tat is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$4$adapted(CompileAndEvaluate.scala:91)\n\tat is.hail.backend.ExecuteContext.$anonfun$scopedExecution$2(ExecuteContext.scala:153)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:99)\n\tat is.hail.backend.ExecuteContext.time(ExecuteContext.scala:174)\n\tat is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1(ExecuteContext.scala:152)\n\tat is.hail.utils.package$.using(package.scala:682)\n\tat is.hail.backend.ExecuteContext.scopedExecution(ExecuteContext.scala:151)\n\tat is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$1(CompileAndEvaluate.scala:91)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:99)\n\tat is.hail.backend.ExecuteContext.time(ExecuteContext.scala:174)\n\tat is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:56)\n\tat is.hail.expr.ir.CompileAndEvaluate$.evalToIR(CompileAndEvaluate.scala:37)\n\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:31)\n\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:60)\n\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:65)\n\tat is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:88)\n\tat is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:39)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:99)\n\tat is.hail.backend.ExecuteContext.time(ExecuteContext.scala:174)\n\tat is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:37)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$2(LoweringPipeline.scala:21)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$2$adapted(LoweringPipeline.scala:19)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:19)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:99)\n\tat is.hail.backend.ExecuteContext.time(ExecuteContext.scala:174)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:10)\n\tat is.hail.expr.ir.lowering.EvalRelationalLets$.$anonfun$apply$2(EvalRelationalLets.scala:15)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:99)\n\tat is.hail.backend.ExecuteContext.time(ExecuteContext.scala:174)\n\tat is.hail.expr.ir.lowering.EvalRelationalLets$.execute$1(EvalRelationalLets.scala:14)\n\tat is.hail.expr.ir.lowering.EvalRelationalLets$.lower$1(EvalRelationalLets.scala:23)\n\tat is.hail.expr.ir.lowering.EvalRelationalLets$.$anonfun$apply$1(EvalRelationalLets.scala:37)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:99)\n\tat is.hail.backend.ExecuteContext.time(ExecuteContext.scala:174)\n\tat is.hail.expr.ir.lowering.EvalRelationalLets$.apply(EvalRelationalLets.scala:12)\n\tat is.hail.expr.ir.lowering.EvalRelationalLetsPass.transform(LoweringPass.scala:175)\n\tat is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:39)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:99)\n\tat is.hail.backend.ExecuteContext.time(ExecuteContext.scala:174)\n\tat is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:37)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$2(LoweringPipeline.scala:21)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$2$adapted(LoweringPipeline.scala:19)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:19)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:99)\n\tat is.hail.backend.ExecuteContext.time(ExecuteContext.scala:174)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:10)\n\tat is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$1(CompileAndEvaluate.scala:57)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:99)\n\tat is.hail.backend.ExecuteContext.time(ExecuteContext.scala:174)\n\tat is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:56)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$execute$1(SparkBackend.scala:430)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:99)\n\tat is.hail.backend.ExecuteContext.time(ExecuteContext.scala:174)\n\tat is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:419)\n\tat is.hail.backend.driver.BackendRpc.$anonfun$runRpc$2(BackendRpc.scala:96)\n\tat is.hail.backend.driver.BackendRpc.withRegisterSerializedFns(BackendRpc.scala:172)\n\tat is.hail.backend.driver.BackendRpc.$anonfun$runRpc$1(BackendRpc.scala:94)\n\tat is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:94)\n\tat is.hail.utils.package$.using(package.scala:682)\n\tat is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:94)\n\tat is.hail.utils.package$.using(package.scala:682)\n\tat is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166)\n\tat is.hail.backend.ExecuteContext$.$anonfun$scoped$1(ExecuteContext.scala:77)\n\tat is.hail.utils.package$.using(package.scala:682)\n\tat is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13)\n\tat is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:76)\n\tat is.hail.backend.driver.Py4JQueryDriver.$anonfun$withExecuteContext$1(Py4JQueryDriver.scala:308)\n\tat is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:16)\n\tat is.hail.backend.driver.Py4JQueryDriver.is$hail$backend$driver$Py4JQueryDriver$$withExecuteContext(Py4JQueryDriver.scala:290)\n\tat is.hail.backend.driver.Py4JQueryDriver$$anon$1$Context$.scoped(Py4JQueryDriver.scala:398)\n\tat is.hail.backend.driver.Py4JQueryDriver$$anon$1$Context$.scoped(Py4JQueryDriver.scala:396)\n\tat is.hail.backend.driver.BackendRpc.runRpc(BackendRpc.scala:80)\n\tat is.hail.backend.driver.BackendRpc.runRpc$(BackendRpc.scala:76)\n\tat is.hail.backend.driver.Py4JQueryDriver$$anon$1.runRpc(Py4JQueryDriver.scala:347)\n\tat is.hail.backend.driver.Py4JQueryDriver$$anon$1.$anonfun$new$1(Py4JQueryDriver.scala:406)\n\tat jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:95)\n\tat jdk.httpserver/sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:82)\n\tat jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:98)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:855)\n\tat jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:95)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:831)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:201)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:561)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:526)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n\n\nHail version: 0.2.136-c32f88309ab0\nError summary: SparkException: Job aborted due to stage failure: Task 1 in stage 7.0 failed 1 times, most recent failure: Lost task 1.0 in stage 7.0 (TID 15) (pimenta executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFatalError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert to pandas\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mvariants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Dataset Summary ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal variants: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m<decorator-gen-1302>:2\u001b[0m, in \u001b[0;36mto_pandas\u001b[0;34m(self, flatten, types)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/gnomad-toolbox/lib/python3.11/site-packages/hail/typecheck/check.py:584\u001b[0m, in \u001b[0;36m_make_dec.<locals>.wrapper\u001b[0;34m(__original_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;129m@decorator\u001b[39m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(__original_func: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, T], \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    583\u001b[0m     args_, kwargs_ \u001b[38;5;241m=\u001b[39m check_all(__original_func, args, kwargs, checkers, is_method\u001b[38;5;241m=\u001b[39mis_method)\n\u001b[0;32m--> 584\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m__original_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gnomad-toolbox/lib/python3.11/site-packages/hail/table.py:4301\u001b[0m, in \u001b[0;36mTable.to_pandas\u001b[0;34m(self, flatten, types)\u001b[0m\n\u001b[1;32m   4299\u001b[0m dtypes_struct \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mrow\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m   4300\u001b[0m collect_dict \u001b[38;5;241m=\u001b[39m {key: hl\u001b[38;5;241m.\u001b[39magg\u001b[38;5;241m.\u001b[39mcollect(value) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m table\u001b[38;5;241m.\u001b[39mrow\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m-> 4301\u001b[0m column_struct_array \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcollect_dict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4302\u001b[0m columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(column_struct_array\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m   4303\u001b[0m data_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m<decorator-gen-1246>:2\u001b[0m, in \u001b[0;36maggregate\u001b[0;34m(self, expr, _localize)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/gnomad-toolbox/lib/python3.11/site-packages/hail/typecheck/check.py:584\u001b[0m, in \u001b[0;36m_make_dec.<locals>.wrapper\u001b[0;34m(__original_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;129m@decorator\u001b[39m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(__original_func: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, T], \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    583\u001b[0m     args_, kwargs_ \u001b[38;5;241m=\u001b[39m check_all(__original_func, args, kwargs, checkers, is_method\u001b[38;5;241m=\u001b[39mis_method)\n\u001b[0;32m--> 584\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m__original_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gnomad-toolbox/lib/python3.11/site-packages/hail/table.py:1911\u001b[0m, in \u001b[0;36mTable.aggregate\u001b[0;34m(self, expr, _localize)\u001b[0m\n\u001b[1;32m   1908\u001b[0m agg_ir \u001b[38;5;241m=\u001b[39m ir\u001b[38;5;241m.\u001b[39mTableAggregate(base\u001b[38;5;241m.\u001b[39m_tir, expr\u001b[38;5;241m.\u001b[39m_ir)\n\u001b[1;32m   1910\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _localize:\n\u001b[0;32m-> 1911\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mEnv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mir\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMakeTuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43magg_ir\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m construct_expr(ir\u001b[38;5;241m.\u001b[39mLiftMeOut(agg_ir), expr\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/anaconda3/envs/gnomad-toolbox/lib/python3.11/site-packages/hail/backend/spark_backend.py:200\u001b[0m, in \u001b[0;36mSparkBackend.execute\u001b[0;34m(self, ir, timed)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m fatal:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfatal\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "File \u001b[0;32m~/anaconda3/envs/gnomad-toolbox/lib/python3.11/site-packages/hail/backend/spark_backend.py:192\u001b[0m, in \u001b[0;36mSparkBackend.execute\u001b[0;34m(self, ir, timed)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, ir: BaseIR, timed: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copy_log_on_error:\n",
      "File \u001b[0;32m~/anaconda3/envs/gnomad-toolbox/lib/python3.11/site-packages/hail/backend/backend.py:229\u001b[0m, in \u001b[0;36mBackend.execute\u001b[0;34m(self, ir, timed)\u001b[0m\n\u001b[1;32m    227\u001b[0m     result, timings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rpc(ActionTag\u001b[38;5;241m.\u001b[39mEXECUTE, payload)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m FatalError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mmaybe_user_error(ir) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ir\u001b[38;5;241m.\u001b[39mtyp \u001b[38;5;241m==\u001b[39m tvoid:\n\u001b[1;32m    231\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gnomad-toolbox/lib/python3.11/site-packages/hail/backend/backend.py:227\u001b[0m, in \u001b[0;36mBackend.execute\u001b[0;34m(self, ir, timed)\u001b[0m\n\u001b[1;32m    221\u001b[0m payload \u001b[38;5;241m=\u001b[39m ExecutePayload(\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_ir(ir),\n\u001b[1;32m    223\u001b[0m     fns\u001b[38;5;241m=\u001b[39m[fn\u001b[38;5;241m.\u001b[39mto_dataclass() \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunctions],\n\u001b[1;32m    224\u001b[0m     stream_codec\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStreamBufferSpec\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    225\u001b[0m )\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 227\u001b[0m     result, timings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rpc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mActionTag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEXECUTE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m FatalError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mmaybe_user_error(ir) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gnomad-toolbox/lib/python3.11/site-packages/hail/backend/py4j_backend.py:270\u001b[0m, in \u001b[0;36mPy4JBackend._rpc\u001b[0;34m(self, action, payload)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[1;32m    269\u001b[0m     error_json \u001b[38;5;241m=\u001b[39m orjson\u001b[38;5;241m.\u001b[39mloads(resp\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m fatal_error_from_java_error_triplet(\n\u001b[1;32m    271\u001b[0m         error_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshort\u001b[39m\u001b[38;5;124m'\u001b[39m], error_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpanded\u001b[39m\u001b[38;5;124m'\u001b[39m], error_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    272\u001b[0m     )\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mcontent, parse_timings(resp\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX-Hail-Timings\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "\u001b[0;31mFatalError\u001b[0m: SparkException: Job aborted due to stage failure: Task 1 in stage 7.0 failed 1 times, most recent failure: Lost task 1.0 in stage 7.0 (TID 15) (pimenta executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\n\nJava stack trace:\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 7.0 failed 1 times, most recent failure: Lost task 1.0 in stage 7.0 (TID 15) (pimenta executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2898)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2834)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2833)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2833)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1253)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1253)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3102)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3036)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3025)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:995)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$parallelizeAndComputeWithIndex$4(SparkBackend.scala:344)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$parallelizeAndComputeWithIndex$4$adapted(SparkBackend.scala:343)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat is.hail.backend.spark.SparkBackend.parallelizeAndComputeWithIndex(SparkBackend.scala:343)\n\tat is.hail.backend.BackendUtils.collectDArray(BackendUtils.scala:85)\n\tat __C4691Compile.apply_region242_242(Emit.scala)\n\tat __C4691Compile.apply_region240_242(Emit.scala)\n\tat __C4691Compile.apply(Emit.scala)\n\tat is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$5(CompileAndEvaluate.scala:93)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:99)\n\tat is.hail.backend.ExecuteContext.time(ExecuteContext.scala:174)\n\tat is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$4(CompileAndEvaluate.scala:93)\n\tat is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$4$adapted(CompileAndEvaluate.scala:91)\n\tat is.hail.backend.ExecuteContext.$anonfun$scopedExecution$2(ExecuteContext.scala:153)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:99)\n\tat is.hail.backend.ExecuteContext.time(ExecuteContext.scala:174)\n\tat is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1(ExecuteContext.scala:152)\n\tat is.hail.utils.package$.using(package.scala:682)\n\tat is.hail.backend.ExecuteContext.scopedExecution(ExecuteContext.scala:151)\n\tat is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$1(CompileAndEvaluate.scala:91)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:99)\n\tat is.hail.backend.ExecuteContext.time(ExecuteContext.scala:174)\n\tat is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:56)\n\tat is.hail.expr.ir.CompileAndEvaluate$.evalToIR(CompileAndEvaluate.scala:37)\n\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:31)\n\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:60)\n\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:65)\n\tat is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:88)\n\tat is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:39)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:99)\n\tat is.hail.backend.ExecuteContext.time(ExecuteContext.scala:174)\n\tat is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:37)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$2(LoweringPipeline.scala:21)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$2$adapted(LoweringPipeline.scala:19)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:19)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:99)\n\tat is.hail.backend.ExecuteContext.time(ExecuteContext.scala:174)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:10)\n\tat is.hail.expr.ir.lowering.EvalRelationalLets$.$anonfun$apply$2(EvalRelationalLets.scala:15)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:99)\n\tat is.hail.backend.ExecuteContext.time(ExecuteContext.scala:174)\n\tat is.hail.expr.ir.lowering.EvalRelationalLets$.execute$1(EvalRelationalLets.scala:14)\n\tat is.hail.expr.ir.lowering.EvalRelationalLets$.lower$1(EvalRelationalLets.scala:23)\n\tat is.hail.expr.ir.lowering.EvalRelationalLets$.$anonfun$apply$1(EvalRelationalLets.scala:37)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:99)\n\tat is.hail.backend.ExecuteContext.time(ExecuteContext.scala:174)\n\tat is.hail.expr.ir.lowering.EvalRelationalLets$.apply(EvalRelationalLets.scala:12)\n\tat is.hail.expr.ir.lowering.EvalRelationalLetsPass.transform(LoweringPass.scala:175)\n\tat is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:39)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:99)\n\tat is.hail.backend.ExecuteContext.time(ExecuteContext.scala:174)\n\tat is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:37)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$2(LoweringPipeline.scala:21)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$2$adapted(LoweringPipeline.scala:19)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:19)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:99)\n\tat is.hail.backend.ExecuteContext.time(ExecuteContext.scala:174)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:10)\n\tat is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$1(CompileAndEvaluate.scala:57)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:99)\n\tat is.hail.backend.ExecuteContext.time(ExecuteContext.scala:174)\n\tat is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:56)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$execute$1(SparkBackend.scala:430)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:99)\n\tat is.hail.backend.ExecuteContext.time(ExecuteContext.scala:174)\n\tat is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:419)\n\tat is.hail.backend.driver.BackendRpc.$anonfun$runRpc$2(BackendRpc.scala:96)\n\tat is.hail.backend.driver.BackendRpc.withRegisterSerializedFns(BackendRpc.scala:172)\n\tat is.hail.backend.driver.BackendRpc.$anonfun$runRpc$1(BackendRpc.scala:94)\n\tat is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:94)\n\tat is.hail.utils.package$.using(package.scala:682)\n\tat is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:94)\n\tat is.hail.utils.package$.using(package.scala:682)\n\tat is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166)\n\tat is.hail.backend.ExecuteContext$.$anonfun$scoped$1(ExecuteContext.scala:77)\n\tat is.hail.utils.package$.using(package.scala:682)\n\tat is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13)\n\tat is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:76)\n\tat is.hail.backend.driver.Py4JQueryDriver.$anonfun$withExecuteContext$1(Py4JQueryDriver.scala:308)\n\tat is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:16)\n\tat is.hail.backend.driver.Py4JQueryDriver.is$hail$backend$driver$Py4JQueryDriver$$withExecuteContext(Py4JQueryDriver.scala:290)\n\tat is.hail.backend.driver.Py4JQueryDriver$$anon$1$Context$.scoped(Py4JQueryDriver.scala:398)\n\tat is.hail.backend.driver.Py4JQueryDriver$$anon$1$Context$.scoped(Py4JQueryDriver.scala:396)\n\tat is.hail.backend.driver.BackendRpc.runRpc(BackendRpc.scala:80)\n\tat is.hail.backend.driver.BackendRpc.runRpc$(BackendRpc.scala:76)\n\tat is.hail.backend.driver.Py4JQueryDriver$$anon$1.runRpc(Py4JQueryDriver.scala:347)\n\tat is.hail.backend.driver.Py4JQueryDriver$$anon$1.$anonfun$new$1(Py4JQueryDriver.scala:406)\n\tat jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:95)\n\tat jdk.httpserver/sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:82)\n\tat jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:98)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:855)\n\tat jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:95)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:831)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:201)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:561)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:526)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n\n\nHail version: 0.2.136-c32f88309ab0\nError summary: SparkException: Job aborted due to stage failure: Task 1 in stage 7.0 failed 1 times, most recent failure: Lost task 1.0 in stage 7.0 (TID 15) (pimenta executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 2]\r"
     ]
    }
   ],
   "source": [
    "# Convert to pandas\n",
    "df = variants.to_pandas()\n",
    "\n",
    "print(\"\\n=== Dataset Summary ===\")\n",
    "print(f\"Total variants: {len(df):,}\")\n",
    "print(f\"Chromosomes: {sorted(df['chrom'].unique())}\")\n",
    "\n",
    "\n",
    "# Show basic statistics for available fields\n",
    "print(\"\\n=== Basic Statistics ===\")\n",
    "if 'AC' in df.columns:\n",
    "    print(f\"AC - Mean: {df['AC'].mean():.2f}, Median: {df['AC'].median():.0f}, Max: {df['AC'].max()}\")\n",
    "if 'AF' in df.columns:\n",
    "    print(f\"AF - Mean: {df['AF'].mean():.6f}, Median: {df['AF'].median():.6f}, Max: {df['AF'].max():.6f}\")\n",
    "if 'AN' in df.columns:\n",
    "    print(f\"AN - Mean: {df['AN'].mean():.2f}, Median: {df['AN'].median():.0f}\")\n",
    "if 'nhomalt' in df.columns:\n",
    "    print(f\"Total Homozygotes: {df['nhomalt'].sum():,}\")\n",
    "\n",
    "print(\"\\nFirst 10 variants:\")\n",
    "print(df.head(10))\n",
    "\n",
    "# Export to TSV for Streamlit\n",
    "output_tsv = '../data/variant_stats.tsv'\n",
    "variants.export(output_tsv)\n",
    "print(f\"\\nExported to: {output_tsv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3487349c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  No population-specific data in this VCF\n",
      "\n",
      "  No prediction scores in this VCF\n"
     ]
    }
   ],
   "source": [
    "# Check for population data\n",
    "pop_fields_present = [col for col in df.columns if any(col.startswith(f'AF_{pop}') for pop in populations)]\n",
    "if pop_fields_present:\n",
    "    print(f\"\\n=== Population-specific data available ===\")\n",
    "    print(f\"Populations: {pop_fields_present}\")\n",
    "else:\n",
    "    print(\"\\n  No population-specific data in this VCF\")\n",
    "\n",
    "# Check for prediction scores\n",
    "pred_fields_present = [col for col in df.columns if col in prediction_fields.keys()]\n",
    "if pred_fields_present:\n",
    "    print(f\"\\n=== Prediction scores available ===\")\n",
    "    print(f\"Scores: {pred_fields_present}\")\n",
    "else:\n",
    "    print(\"\\n  No prediction scores in this VCF\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnomad-toolbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
